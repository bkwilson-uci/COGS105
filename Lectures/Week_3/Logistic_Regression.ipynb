{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5cffbf",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "* The first classifier we will discuss in this class is  **Logistic Regression**. \n",
    "\n",
    "* In Linear Regression, we fit a line to data. \n",
    "\n",
    "* In a simple (two-class) Logistic Regression we will fit a curve to the probability that the data comes from one **class**\n",
    "\n",
    "* Many AI models are complicated versions of logistic regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9dd6f",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Exam_pass_logistic_curve.svg.png\" alt=\"alt text\" width=\"50%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79a064",
   "metadata": {},
   "source": [
    "### Logistic Function \n",
    "\n",
    "Logistic Regression addresses the problem of estimating a probability model, $ùëÉ(Y = 1|x)$. \n",
    "\n",
    "The logistic regression model uses a function for the probability model, called the logistic function:\n",
    "\n",
    "$$ P(Y = 1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def logistic(x, beta0=0, beta1=1):\n",
    "    p =  1 / (1 + np.exp(-(beta0 + beta1 * x)))\n",
    "    return p\n",
    "\n",
    "# Generate a range of x values\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Parameters\n",
    "beta0 = 0   # Intercept\n",
    "beta1 = 1   # Slope\n",
    "\n",
    "# Compute logistic values\n",
    "y = logistic(x, beta0, beta1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label=fr'Logistic: $\\beta_0={beta0}$, $\\beta_1={beta1}$')\n",
    "plt.title(\"Logistic Function\", fontsize=14)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"P(Y=1|x)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55783245",
   "metadata": {},
   "source": [
    "The probability model will predict $ùëÉ(Y = 1 | x)$ with an S-shaped curve:\n",
    "\n",
    "* $\\beta_0$ shifts the curve right or left by $c = \\frac{-\\beta_0}{\\beta_1}$ \n",
    "* $\\beta_1$ controls the steepness the S-shaped curve. Distance from ¬Ω to almost 1 or ¬Ω to\n",
    "almost 0 to ¬Ω is $\\frac{2\\beta_1}$\n",
    "* if $\\beta_1$ is positive, then the predicted $P(Y = 1|x)$ goes from zero for small values\n",
    "of $x$ to one for large values of $X$\n",
    "if $\\beta_1$ is negative, then the predicted $P(Y = 1|x)$ goes from one for small values\n",
    "of $x$ to zero for large values of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be0014",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"logistic.png\" alt=\"alt text\" width=\"50%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261a73c",
   "metadata": {},
   "source": [
    "* It's useful to rewrite the logistic regression model, in terms of odds.  This is called the **logit** function by statisticians and in economics\n",
    "\n",
    "\n",
    "$$ \\text{logit}\\big(P(Y = 1 \\mid x)\\big) = \\ln\\left( \\frac{P(Y = 1 \\mid x)}{1 - P(Y = 1 \\mid x)} \\right) = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "* The ratio shown is the **odds** ratio between the probability of $Y = 1$ with the probability $Y = 0$, where $Y$ can only be 1 or 0  \n",
    "\n",
    "* A one unit change in x is associated with an $e^{\\beta_1}$ change in the odds that $ùëå = 1$ .\n",
    "\n",
    "* What happens with the odds ratio is 1, i.e., $P(Y = 1) = 0.5?$\n",
    "\n",
    "* Since $P(Y = 0) = 1 - P(Y = 1) = 0.5$, \n",
    "\n",
    "$$ \\ln\\left( \\frac{P(Y = 1)}{1 - P(Y = 1)} \\right) = ln (1) = 0  = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "$$ x = -\\frac{\\beta_0}{\\beta_1} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef0a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a range of x values\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Parameters\n",
    "beta0 = 2   # Intercept\n",
    "beta1 = 1   # Slope\n",
    "c = -beta0 / beta1  # x value where P(Y=1|x) = 0.5  \n",
    "# Compute logistic values\n",
    "y = logistic(x, beta0, beta1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label=fr'Logistic: $\\beta_0={beta0}$, $\\beta_1={beta1}$')\n",
    "plt.plot([c,c], [0 ,1], 'r-',label = 'Decision Boundary')  # Point where P(Y=1|x) = 0.5\n",
    "plt.title(\"Logistic Function\", fontsize=14)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"P(Y=1|x)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234c328",
   "metadata": {},
   "source": [
    "### Probability mass function for logistic regression \n",
    "\n",
    "* In logistic regression, the response variable $Y$ is binary, taking values in\n",
    "\n",
    "$$ Y \\in \\{0, 1\\} $$\n",
    "\n",
    "\n",
    "* We define\n",
    "\n",
    "$$     P(Y = 1 \\mid x) = p \\quad \\text{and} \\quad P(Y = 0 \\mid x) = 1 - p $$\n",
    "\n",
    "\n",
    "* $p$ is modeled using the logistic function:\n",
    "\n",
    "$$ p = \\frac{1}{1 + e^{- (\\beta_0 + \\beta_1 x)}} $$\n",
    "\n",
    "\n",
    "* The probability mass function (PMF) of a Bernoulli random variable can be written compactly as\n",
    "\n",
    "$$P(Y = y \\mid x) = p^{\\,y} (1 - p)^{\\,1 - y}, \\quad \\text{for } y \\in \\{0,1\\} $$\n",
    "\n",
    "\n",
    "* This expression encodes both possible outcomes in a single formula. Specifically:\n",
    "    * If $y = 1$\n",
    "    $$ P(Y = 1 \\mid x) = p^1 (1 - p)^0 = p $$\n",
    "    * If $y = 0$\n",
    "    $$ P(Y = 0 \\mid x) = p^0 (1 - p)^1 = 1 - p $$\n",
    "\n",
    "\n",
    "* The notation $P(Y = y)$ means *the probability that the random variable $Y$ takes the specific observed value $y$*. Since $y$ can only be $0$ or $1$, this single expression\n",
    "\n",
    "$$ P(Y = y) = p^{\\,y}(1-p)^{\\,1-y} $$\n",
    "\n",
    "automatically selects the correct probability term depending on whether the observed outcome was $0$ or $1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645ae71",
   "metadata": {},
   "source": [
    "### Likelihood for Logistic Regression\n",
    "\n",
    "* Given a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with $y_i \\in \\{0,1\\}$ and\n",
    "\n",
    "$$ p_i = P(Y_i = 1 \\mid x_i) = \\sigma(\\beta_0 + \\beta_1 x_i) = \\frac{1}{1 + e^{- (\\beta_0 + \\beta_1 x_i)}} $$\n",
    "\n",
    "* the Likelihood of the parameter vector $\\beta = (\\beta_0, \\beta_1)$ is\n",
    "\n",
    "$$    L(\\beta \\mid x_{1:N}, y_{1:N}) = \\prod_{i=1}^N p_i^{\\,y_i} (1 - p_i)^{\\,1 - y_i} $$\n",
    "\n",
    "* Taking the logarithm yields the log-likelihood:\n",
    "\n",
    "$$ \\ell(\\beta) = \\ln L(\\beta) = \\sum_{i=1}^N \\left[ y_i \\ln(p_i) + (1 - y_i) \\ln(1 - p_i) \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ace317",
   "metadata": {},
   "source": [
    "### Loss Function \n",
    "\n",
    "* In machine learning, the term loss function is used to refer to some measure of error that you are trying to minimize.  If you have a probability model, as in Logistic Regression,  the loss function is simply the **negative log-Likelihood** \n",
    "\n",
    "$$ -\\ell(\\beta) = -\\ln L(\\beta) = -\\sum_{i=1}^N \\left[ y_i \\ln(p_i) + (1 - y_i) \\ln(1 - p_i) \\right] $$\n",
    "\n",
    "* As mentioned in the video lecture, in machine learning this is called negative cross-entropy.  \n",
    "* How do we minimize this? Differentiate, equate to zero and solve for it!\n",
    "* Or, stick into some numerical procedure (gradient descent usually) to find the minimum  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad05fc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdcacf09",
   "metadata": {},
   "source": [
    "### An example of real data:  \n",
    "\n",
    "## Diabetes Prediction Example \n",
    "[Pima Indians Diabetes Study](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e07858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"../data/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463eba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954353c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I grabbed a list of all the columns \n",
    "cols = pima.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87572ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine how many of each outcome\n",
    "pima[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407e9e4",
   "metadata": {},
   "source": [
    "* I always like to take a first glance at all the data.  This works for continuous valued data, and works if you have fewer than 10 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pima, hue=\"Outcome\", height=3);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2170827",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pima['Outcome']\n",
    "predictors = pima['Glucose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed16157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(predictors, diabetes,'ro', alpha=0.7)\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('Diabetes (0 = No, 1 = Yes)')\n",
    "plt.title('Diabetes Outcome vs Glucose Level')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b38ff",
   "metadata": {},
   "source": [
    "* First step is always to set aside some data for testing after we train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train, predictors_test, diabetes_train, diabetes_test = train_test_split(predictors, diabetes, test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6714b2",
   "metadata": {},
   "source": [
    "* Lets examine how the test and training data are distributed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(diabetes_train == 1), np.sum(diabetes_train == 0))\n",
    "print(np.sum(diabetes_test== 1), np.sum(diabetes_test == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af57d9",
   "metadata": {},
   "source": [
    "* our syntax is sklearn is standard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a388039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to do a reshape here because I have a single predictor.\n",
    "predictors_train.values.reshape(-1,1)\n",
    "# this forces it to be a 2D array with one column and many rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c45bd2",
   "metadata": {},
   "source": [
    "* first lets fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(predictors_train.values.reshape(-1,1), diabetes_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f095b3",
   "metadata": {},
   "source": [
    "* Now lets evaluate the model performance.  the `score` method returns accuracy for Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = lr.score(predictors_train.values.reshape(-1,1), diabetes_train)\n",
    "accuracy_test = lr.score(predictors_test.values.reshape(-1,1), diabetes_test)\n",
    "print(f\"Training Accuracy: {accuracy_train:.3f}\")\n",
    "print(f\"Test Accuracy: {accuracy_test:.3f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191573fe",
   "metadata": {},
   "source": [
    "* Is that good?\n",
    "* Is there anything unusual?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823683b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83cc0293",
   "metadata": {},
   "source": [
    "*Whats the model it generated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863abd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = list()\n",
    "beta.append(lr.intercept_[0])\n",
    "beta.append(lr.coef_[0][0])\n",
    "print(f\"beta0 (intercept): {beta[0]:.3f}\")\n",
    "print(f\"beta1 (slope): {beta[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586afae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary = -beta[0] / beta[1]\n",
    "print(f\"Decision Boundary (Glucose level where P(Y=1|x)=0.5): {decision_boundary:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a1f2a",
   "metadata": {},
   "source": [
    "### Confusion Matrix \n",
    "\n",
    "* A Confusion Matrix provides better insight into classifier performance than simple accuracy \n",
    "* To obtain a confusion matrix we need predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_train_pred = lr.predict(predictors_train.values.reshape(-1,1))\n",
    "diabetes_test_pred = lr.predict(predictors_test.values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187f0bd",
   "metadata": {},
   "source": [
    "Compute the confusion matrix for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d68ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_train = confusion_matrix(diabetes_train, diabetes_train_pred)\n",
    "print(cnf_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Never say '\"Healthy\", \"Normal\", just say \"Undiagnosed\"\n",
    "class_names=['Undiagnosed','Diabetes'] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_train), annot=True, cmap=\"jet\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.xticks(tick_marks+0.5, class_names)\n",
    "plt.yticks(tick_marks+0.5, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b626a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_test = confusion_matrix(diabetes_test, diabetes_test_pred)\n",
    "#Never say '\"Healthy\", \"Normal\", just say \"Undiagnosed\"\n",
    "class_names=['Undiagnosed','Diabetes'] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_test), annot=True, cmap=\"jet\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.xticks(tick_marks+0.5, class_names)\n",
    "plt.yticks(tick_marks+0.5, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af764d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
